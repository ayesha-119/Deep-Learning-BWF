{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsks2dw6PhnIKAiRfr8CeT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayesha-119/Deep-Learning-BWF/blob/master/Task_27.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”´ Task 27**\n",
        "\n",
        "**Topics: Stacking Up Layers in a Neural Network (ANN)**\n",
        "\n",
        "Resources: https://drive.google.com/file/d/1i9dPxM_1M4HYN5bYxFcuklC1vM0GrOCq/view?usp=share_link"
      ],
      "metadata": {
        "id": "OkU-YYQMMIus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking up layers in a neural network, also known as an Artificial Neural Network (ANN), refers to the process of adding multiple layers to create a deep learning model. Each layer in the network performs specific computations and contributes to the overall learning and representation capabilities of the model. Here's an overview of the process:\n",
        "\n",
        "1.  **Input Layer:**\n",
        "\n",
        "*   The input layer receives the input data and passes it to the next layer.\n",
        "*   The number of neurons in the input layer is determined by the dimensions of the input data.\n",
        "\n",
        "2.  **Hidden Layers:**\n",
        "\n",
        "\n",
        "*   Hidden layers are intermediate layers between the input and output layers.\n",
        "\n",
        "*   Each hidden layer consists of multiple neurons (nodes) that perform computations on the input data.\n",
        "*   The number of hidden layers and the number of neurons in each layer are design choices and can vary depending on the complexity of the problem.\n",
        "\n",
        "3.  **Activation Functions:**\n",
        "\n",
        "\n",
        "\n",
        "*   Activation functions are applied to the output of each neuron in a layer.\n",
        "\n",
        "*   Activation functions introduce non-linearity into the network, enabling the model to learn complex relationships in the data.\n",
        "*   Common activation functions include sigmoid, tanh, ReLU, Leaky ReLU, and softmax, depending on the task and layer.\n",
        "\n",
        "4.  **Output Layer:**\n",
        "*   The output layer produces the final predictions or outputs of the model.\n",
        "\n",
        "*   The number of neurons in the output layer depends on the type of problem: binary classification, multi-class classification, or regression.\n",
        "*   The activation function used in the output layer is chosen based on the problem type and desired output format.\n",
        "5.  **Forward Propagation:**\n",
        "*   During forward propagation, the input data flows through the network layer by layer, with computations and activations being applied at each layer.\n",
        "*   The output of one layer serves as the input to the next layer until reaching the output layer.\n",
        "6.  **Backpropagation and Training:**\n",
        "\n",
        "\n",
        "*   Backpropagation is used to compute the gradients of the loss function with respect to the network parameters.\n",
        "\n",
        "*   The gradients are then used by the optimizer to update the weights and biases of the network during training.\n",
        "*   Training involves iteratively feeding the training data through the network, adjusting the weights based on the computed gradients, and optimizing the model's performance.\n",
        "\n",
        "\n",
        "By stacking up multiple layers in a neural network, deep learning models can learn complex representations and patterns in the data. Deep architectures allow for hierarchical feature learning and have shown excellent performance in various domains such as computer vision, natural language processing, and speech recognition.\n",
        "\n",
        "\n",
        "Experimenting with different architectures, activation functions, and layer sizes can help optimize the model's performance for specific tasks. It's essential to consider the complexity of the problem, the amount of available data, and the computational resources when designing the architecture of a neural network.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_glGwfbTnr99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtrwtcSeJEOR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Add the input layer\n",
        "model.add(keras.layers.Dense(units=64, activation='relu', input_shape=(input_dim,)))\n",
        "\n",
        "# Add additional hidden layers\n",
        "model.add(keras.layers.Dense(units=128, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Add the output layer\n",
        "model.add(keras.layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n"
      ]
    }
  ]
}