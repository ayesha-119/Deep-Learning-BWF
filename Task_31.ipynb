{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrwu1GbOOX0ry+KuYbORnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayesha-119/Deep-Learning-BWF/blob/master/Task_31.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”´ Task 31**\n",
        "\n",
        "**Topics: Applying One-Hot Encoding and Word Embeddings**\n",
        "\n",
        "\n",
        "Resource: https://drive.google.com/file/d/1i9dPxM_1M4HYN5bYxFcuklC1vM0GrOCq/view?usp=share_link"
      ],
      "metadata": {
        "id": "w689FBifNarq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to follow:\n",
        "\n",
        "Convert the text to Lower Case\n",
        "\n",
        "Word Tokenize\n",
        "\n",
        "Get its integer value i.e the position by using LabelEncoder()\n",
        "\n",
        "Get one hot encoding of the word by referring to the label encoded values by using to_categorical()"
      ],
      "metadata": {
        "id": "66eSiUa_bYm-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XDZCrIM9JeVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08b4ab3-c402-4db6-fd46-81771d37e7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "text_list  = ['the cat sat on the mat','The dog ate my chocolate']\n",
        "tokenizer = Tokenizer(num_words= 1000)\n",
        "# builds word index\n",
        "tokenizer.fit_on_texts(text_list)\n",
        "\n",
        "# turns text into list of integer indices\n",
        "tokenizer.texts_to_sequences(text_list)\n",
        "\n",
        "one_hot_vectors = tokenizer.texts_to_matrix(text_list,mode = 'binary')\n",
        "print(one_hot_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train our own word embedding on data for our problem**"
      ],
      "metadata": {
        "id": "o_8DLHXneSx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "        'Great effort',\n",
        "        'nice work',\n",
        "        'Excellent!',\n",
        "'Weak','Poor effort!',\n",
        "'not good',\n",
        "'poor work',\n",
        "'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "UKM0Umzbdtt6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode the documents\n",
        "from keras.preprocessing.text import one_hot\n",
        "# keras.preprocessing.text.one_hot(text, n, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAsN77USdwFI",
        "outputId": "0f056803-e4f8-420b-a596-fe14d6dd4900"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14, 6], [21, 7], [24, 24], [18, 7], [13], [14], [40, 24], [23, 21], [40, 7], [36, 22, 6, 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding sequence\n",
        "from keras.utils import pad_sequences\n",
        "# keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuytKKZXdzHt",
        "outputId": "77aa17d5-3a15-47fb-f977-f5e9efb8fdf0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14  6  0  0]\n",
            " [21  7  0  0]\n",
            " [24 24  0  0]\n",
            " [18  7  0  0]\n",
            " [13  0  0  0]\n",
            " [14  0  0  0]\n",
            " [40 24  0  0]\n",
            " [23 21  0  0]\n",
            " [40  7  0  0]\n",
            " [36 22  6 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Embedding layer in keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten,Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length = max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJPLrao2eKKf",
        "outputId": "c91a5fc0-c1ad-467b-9922-5d5210d51e4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fccb91250f0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}